---
title: 服务治理技术概览：服务发现、负载均衡、限流、熔断降级等
date: 2018-08-26
tags: 
    - 微服务
---

[TOC]

现在都在讲微服务，那么微服务到底是是什么呢，微服务的定义：

> 开发单个应用作为一系列小型服务的套件，其中每个服务都运行在自己的进程中，并且通过轻量级的机制实现彼此间的通信，这通常是HTTP资源API。这些服务是围绕着业务功能构建的，并且可以通过完全自动化的部署机制进行独立部署。这些服务的集中式管理做到了最小化，每一种服务都可以通过不同的编程语言进行编写，并且可以使用不同的数据存储技术。
>
> -- 来自[微服务](http://martinfowler.com/articles/microservices.html)，作者[James Lewis](https://twitter.com/boicy)与[Martin Fowler](http://www.martinfowler.com/)

毫无疑问，微服务带来了很多的好处：

1. 服务时松散耦合的
2. 每个服务专注于某个业务模块
3. 每个服务可以通过不同语言编写，也可以使用不同的存储
4. 每个服务可以独立部署和扩展

但是，微服务由于把系统拆分成许多子服务，也带来了很多问题：

1. 运维监控成本加大
2. 服务之间的调用增加了网络开销
3. 服务调用路线的跟踪
4. 调试和测试困难

为了解决微服务带来的各种问题，或者说为了保障微服务的正常工作，提出了服务治理（[SOA Governance](https://en.wikipedia.org/wiki/SOA_governance)）的概念，服务治理就是通过一些技术手段保障微服务的正常工作，主要包括服务发现，负载均衡，请求路由，限流，降级和熔断等。

## 服务发现

服务发现的主要好处是「零配置」：不用使用硬编码的网络地址，只需服务的名字（有时甚至连名字都不用）就能使用服务。

常用的有etcd等。

...

## 负载均衡

当系统面临大量用户访问，负载过高的时候，通常会使用增加服务器数量来进行横向扩展，使用集群和负载均衡提高整个系统的处理能力。

### 实现方式

注：负载均衡功能可以直接做到硬件里面，这里只讨论软件实现的负载均衡。

从具体实现来看，可以分为软负载均衡和反向代理：

- 软负载均衡：不会经过中间代理，性能较好，但无法做流量精细化控制，这类有DNS负载均衡和iptables等
- 反向代理：通过代理来做负载均衡，该代理就专门的负载均衡服务器（LB，Load Balancer）。由于流量都会过LB，因此可以做到比较精细的流量分发，但是LB本身容易成为瓶颈。

从网络层来看，主流的工作在四层和七层：

- 四层：OSI的传输层，通过修改网络包的ip地址和端口来转发，比如LVS。
- 七层：OSI的应用层，可以将HTTP请求发送到具体的应用服务器，比如nginx, haproxy。

#### DNS负载均衡

DNS负责将用户请求的域名映射为实际的IP地址，这种映射可以是一对多的（ DNS的A记录，用来指定域名对应的IP地址），这样DNS服务器便充当负载均衡调度器。

DNS节省了所谓的主站点，DNS服务器已经充当了主站点的职能。常见的策略是对多个A记录进行RR(轮询)。

大型网站一般使用DNS作为第一级负载均衡，这种做法缺点是DNS生效时间略长（各地ISP服务商刷新域名DNS的时间不一致），扩展性差。

#### IP负载均衡（四层）

工作在OSI的传输层，原理是在Linux内核态获取到IP报文后，修改网络包的ip地址和端口，根据特定的负载均衡算法将IP报文转发到集群的某台服务器中去。这里的IP报文转发，还涉及到转发模式的问题，比如NAT模式。

比较有代表性并且被大量使用的的就是LVS了，缺点依赖Linux内核的网络性能。Linux网络协议栈路径很长，到达LVS时，已经经过了一段很长的协议栈处理，但是这段处理对于LVS来说都不是必需的，这也造成了一部分不必要的性能损耗。可以考虑使用DPDK。

#### 反向代理负载均衡（七层）

反向代理服务器的核心工作是转发HTTP请求，它工作在HTTP层面，因此，基于反向代理的负载均衡也称为七层负载均衡。任何对于实际服务器的HTTP请求都必须经过调度器；调度器必须等待实际服务器的HTTP响应，并将它反馈给用户。

常见的有nginx, haproxy等。

相比之下，四层负载架构设计比较简单，无需解析具体的消息内容，在网络吞吐量及处理能力上会相对比较高，而七层负载均衡的优势则体现在功能多，控制灵活强大。

### 算法

#### 轮询（Round Robin）

当服务器群中各服务器的处理能力相同时，且每笔业务处理量差异不大时，最适合使用这种算法。 

原理是每一次把来自用户的请求轮流分配给内部中的服务器，从1开始，直到N(内部服务器个数)，然后重新开始循环。

#### 一致hash

考虑通常的hash算法都是将value 映射到一个32位的key值，也就是（0~2的32次方-1）数值空间，我们可以将这个空间想象一个首（0） 一个尾（2的32次方-1）。

将各Node的key采用hash计算，可得到各个节点的hash值，映射到环上。一般node的key为ip地址。

同样的，每次请求到来的时候，把请求的ip地址进行hash，映射到环上，并且顺时针方向寻得离其最近的节点即为其服务节点。

这样每个节点覆盖了圆上从上一节点到其本身的一段弧段区间。如某一节点失效，之前落入其弧段区间的请求即会顺时针移到与其相邻的节点。

为了增加节点的均匀分散，还加入了虚拟节点的概念，就是每个服务节点除了它本身之外，还设置了多个它的副本节点，请求顺时针寻找节点的时候，如果找的是某个节点的虚拟节点，那么请求还是落到了该节点上。

![consistent hash](https://i.imgur.com/UkRyEJt.png)

一致hash最大的优势在于，新增或者删除一台服务器的时候，不会引起已有服务器的cache大规模失效和转移。并且虚拟节点的加入保证了请求的均衡分布。

注意：节点和请求的hash都是同一个hash算法。一般的，节点可以采用ip作为key，对于虚拟节点，则可以采用ip+后缀的形式作为key。对于请求，可以使用host+uri作为key。

## 限流

作为服务提供方，你无法限制调用方如何调用自己，因此需要设置保护机制：比如限制 A 调用方对某个服务的某个的接口的每秒最大请求次数。

常见的限流算法一般包括：时间窗口（固定或滑动）、令牌桶算法、漏桶算法和分布式限流等。

### 时间窗口

#### 固定时间窗口

基于固定时间窗口的限流算法是非常简单的。首先选定一个时间窗口，之后每次接口请求到来都累加计数器，如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许 100 次接口请求），累加访问次数超过限流值，则限流熔断拒绝接口请求。当进入下一个时间窗口之后，计数器清零重新计数。

这种做法的缺点很明显：限流策略过于粗糙，无法对两个时间窗口交界处附近的突发流量进行限流。

#### 滑动时间窗口

滑动时间窗口算法是对固定时间窗口算法的一种改进，流量经过滑动时间窗口算法整形之后，可以保证任意时间窗口内，都不会超过限流值，从流量曲线上来看会更加平滑，可以部分解决上面提到的窗口交界处突发流量问题。

未完待续...

对于时间窗口限流算法，还有很多改进版本，比如：

多层次限流，我们可以对同一个接口设置多条限流规则，除了 1 秒不超过 100 次之外，我们还可以设置 100ms 不超过 20 次 (这里需要设置的比 10 次大一些)，两条规则同时限制，流量会更加平滑。

### 令牌桶和漏桶

这里介绍两种更加平滑的限流算法：令牌桶算法和漏桶算法，这两种算法是最常用的限流算法。

#### 令牌桶

令牌桶算法：

1. 接口限制 t 秒内最大访问次数为 n，则每隔 t/n 秒会放一个 token 到桶中；
2. 桶中最多可以存放 b 个 token，如果 token 到达时令牌桶已经满了，那么这个 token 会被丢弃；
3. 接口请求会先从令牌桶中取 token，拿到 token 则处理接口请求，拿不到 token 则执行限流。

可以看出，令牌桶算法的特点：既可以控制某个时间窗口内进入系统的请求请求量，同时也允许突发流量。

举个例子：

> 在秒杀活动中，用户的请求速率是不固定的，这里我们假定为10r/s，令牌按照5个每秒的速率放入令牌桶，桶中最多存放20个令牌，那系统就只会允许持续的每秒处理5 个请求，或者每隔4 秒，等桶中20 个令牌攒满后，一次处理20个请求的突发情况，保证系统稳定性。

令牌桶算法看似比较复杂，每间隔固定时间都要放 token 到桶中，但可以采用lazy模式来放token。每次在取 token 之前，根据上次放入 token 的时间戳和现在的时间戳，计算出这段时间需要放多少 token 进去，一次性放进去。

伪代码：

```
rate = 5; // unit: messages
per  = 1; // unit: seconds
allowance = rate; // unit: messages 初始化
last_check = now(); // floating-point, e.g. usec accuracy. Unit: seconds

when (message_received):
    current = now();
    time_passed = current - last_check;
    last_check = current;
    allowance += time_passed * (rate / per); //一次性放入token（自上次消费到现在的token总数）
    
    if (allowance > rate):
        allowance = rate; // throttle
    
    if (allowance < 1):
        discard_message();
    else:
        forward_message();
        allowance = allowance - 1;
```

#### 漏桶

漏桶算法是基于令牌桶算法的一个改进，唯一一点不同：对于取令牌的频率也有限制，要按照 t/n 固定的速度来取令牌。所以漏桶算法相对来说流量更加平滑，突发流量会被限流。

还有一些其他基于令牌桶的改进算法：

1. 预热桶
2. 一次性放入多个令牌
3. 支持一次性取多个令牌

### 分布式限流算法

相对于单机限流算法，分布式限流算法的是指: 算法可以分布式部署在多台机器上面，多台机器协同提供限流功能，可以对同一接口或者服务做限流。这里需要提供计算接口请求数的中心计数器，一般可以采用Redis。

但是中心计数器容易成为瓶颈，毕竟多了一次RPC调用。

### 总结

相比基于时间窗口的限流算法，令牌桶和漏桶算法对流量平滑效果比时间窗口算法要好很多，但是并不是平滑效果越好就越合适，对于没有提前预热的令牌桶，如果做否决式限流，会导致误杀很多请求。可能导致流量在细时间粒度上面都很平滑，但是误杀了很多本不应该拒绝的接口请求。

所以令牌桶和漏桶算法比较适合阻塞式限流，比如一些后台 job 类的限流，超过了最大访问频率之后，请求并不会被拒绝，而是会被阻塞到有令牌后再继续执行。

对于像微服务接口这种对响应时间比较敏感的限流场景，会比较适合选择基于时间窗口的否决式限流算法，其中滑动时间窗口限流算法空间复杂度较高，内存占用会比较多。所以对比来看，尽管固定时间窗口算法处理临界突发流量的能力较差，但实现简单，而简单带来了好的性能和不容易出错，所以固定时间窗口算法也不失是一个好的微服务接口限流算法。

单机限流是指：独立的对集群中的每台实例进行接口限流，比如限制每台实例接口访问的频率为最大 1000 次 / 秒，单机限流一般使用单机限流算法；

分布式限流是指：提供服务级的限流，限制对微服务集群的访问频率，比如限制 A 调用方每分钟最多请求 1 万次“用户服务”，分布式限流既可以使用单机限流算法也可以使用分布式限流算法。

单机限流的初衷是防止突发流量压垮服务器，所以比较适合针对并发做限制。从防止某调用方过度竞争服务资源来说，分布式限流更加适合。

## 熔断降级

### 熔断

一个面向用户端的API，服务内部的RPC调用链可能会非常长。这会造成以下几个问题：

- API接口可用性降低：内部依赖30个服务，每个服务可用性为99.99%，那整个的可用性就是：99.99%的30次方 ＝ 99.7%
- 系统被block：一个请求的调用链上面有10个服务，只要这10个服务中有1个超时，就会导致这个请求超时。

为了解决上述问题，服务熔断的思想被提出来：类似现实世界中的“保险丝“，当某个异常条件被触发，直接熔断整个服务，而不是一直等到此服务超时。 

熔断的触发条件可以依据不同的场景有所不同，比如统计一个时间窗口内失败的调用次数。实际开发中，经常使用Netflix开源的Hystrix框架，通过参数对熔断进行配置：滑动窗口大小，再次检测开启时间，错误率等。

这三者值分别为20，5000，50%，则表示：

每当20个请求中，有50%失败时，熔断器就会打开，此时再调用此服务，将会直接返回失败，不再调远程服务。直到5s钟之后，重新检测该触发条件，判断是否把熔断器关闭，或者继续打开。

### 降级

服务降级本质上是降低了部分服务的处理能力，增强另一部分服务处理能力，而访问量不变。

服务降级在操作上，是前后端联动，相互配合来做到的，意味着，在代码设计阶段，前后端必须要共同考虑服务降级的方案。根据事态的严重性，会制定不能级别的降级方案：

1. 按比例执行API：预先设定一定的比例，将这部分流量带来的API请求，不做处理，直接返回默认值，其余请求能继续正常返回。
2. 关闭非核心服务API：前端页面能继续访问，但是将与核心功能无关的API关闭掉，保证主流程能继续执行，前端隐藏对应的信息展示。
3. 延迟返回，结果转异步返回：页面能正常访问，但是涉及到记录变更，会提示稍晚更新结果，将数据记录更新的返回转到异步MQ。
4. 将前端页面切到静态页：通过Nginx设置，将页面跳转到一个静态页面。例如“目前系统正在维护，blabla”这样的页面。

关于如何使用Hystrix做降级，此处不详述，参见官网。

## 参考

[微服务接口限流的设计与思考（附GitHub框架源码）](http://www.infoq.com/cn/articles/microservice-interface-rate-limit)