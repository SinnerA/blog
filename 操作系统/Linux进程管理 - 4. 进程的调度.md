---
title: Linux进程管理 - 4. 进程的调度
date: 2018-05-20
tags:
    - Linux
    - 进程
---

[TOC]

## NICE值

nice值应该是熟悉Linux/UNIX的人很了解的概念了，我们都知它是反应一个进程“优先级”状态的值，其取值范围是-20至19，一共40个级别。**这个值越小，表示进程”优先级”越高，而值越大“优先级”越低。**我们可以通过nice命令来对一个将要执行的命令进行nice值设置，方法是：

```shell
[root@zorrozou-pc0 zorro]# nice -n 10 bash
```

默认情况下，进程的优先级应该是从父进程继承来的，这个值一般是0。可以使用比如top、ps等命令查看进程的nice值。

需要大家注意的是，我在这里都在使用nice值这一称谓，而非优先级priority这个说法。要强调一下，不要混淆了系统中的这两个概念，nice值能影响优先级，但**两者不等价**。

```shell
[root@zorrozou-pc0 zorro]# ps -l
F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD
4 S     0  6924  5776  0  80   0 - 17952 poll_s pts/5    00:00:00 sudo
4 S     0  6925  6924  0  80   0 -  4435 wait   pts/5    00:00:00 bash
0 R     0 12971  6925  0  80   0 -  8514 -      pts/5    00:00:00 ps
```

上面PRI和NI分别代表进程的优先级和nice值。

理解：**越nice的人抢占资源的能力就越差，而越不nice的人抢占能力就越强。**

在原来使用O1调度的Linux上，我们还会把**nice值叫做静态优先级**，这也基本符合nice值的特点，就是当nice值设定好了之后，除非我们用renice去改它，否则它是不变的。而priority的值在之前内核的O1调度器上表现是会变化的，所以也叫做**动态优先级**。

## 优先级和实时进程

在内核中，进程优先级的取值范围是通过一个宏定义的，这个宏的名称是MAX_PRIO，它的值为140。而这个值又是由另外两个值相加组成的，一个是代表nice值取值范围的NICE_WIDTH宏，另一个是代表实时进程realtime优先级范围的MAX_RT_PRIO宏。

说白了就是，Linux实际上实现了140个优先级范围，取值范围是从0-139，这个值越小，优先级越高。nice值的-20到19，映射到实际的优先级范围是100-139。新产生进程的默认优先级被定义为：

```shell
#define DEFAULT_PRIO            (MAX_RT_PRIO + NICE_WIDTH / 2)
```

实际上对应的就是nice值的0。正常情况下，任何一个进程的优先级都是这个值，即使我们通过nice和renice命令调整了进程的优先级，它的取值范围也不会超出100-139的范围，除非这个进程是一个实时进程，那么它的优先级取值才会变成0-99这个范围中的一个。这里隐含了一个信息，就是说当前的Linux是一种已经支持实时进程的操作系统。

什么是实时操作系统，我就不再这里详细解释其含义以及在工业领域的应用了，有兴趣的可以参考一下[实时操作系统的维基百科](https://zh.wikipedia.org/wiki/%E5%AE%9E%E6%97%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F)。简单来说，实时操作系统需要保证相关的实时进程在较短的时间内响应，不会有较长的延时，并且要求最小的中断延时和进程切换延时。对于这样的需求，一般的进程调度算法，无论是O1还是CFS都是无法满足的，所以**内核在设计的时候，将实时进程单独映射了100个优先级，这些优先级都要高与正常进程的优先级（nice值更小）**，而实时进程的调度算法也不同，它们采用更简单的调度算法来减少调度开销。总的来说，Linux系统中运行的进程可以分成两类：

1. 非实时进程：

   - 优先级100-139

   - 调度策略：

     **SCHED_OTHER用于普通进程，通过CFS调度器实现**。还有其它两种，SCHED_BATCH用于非交互的处理器消耗型进程。SCHED_IDLE是在系统负载很低时使用。

2. 实时进程：

   - 优先级0-99

   - 调度策略：

     优先级高的（就是priority数字小的）进程一定会保证先于优先级低的进程执行，当优先级一样时，采取以下调度策略：

     SCHED_FIFO:以先进先出的队列方式进行调度，在优先级一样的情况下，谁先执行的就先调度谁，除非它退出或者主动释放CPU。

     SCHED_RR:以时间片轮转的方式对相同优先级的多个进程进行处理。时间片长度为100ms。 

## 调度算法

对于非实时进程优先级的处理，介绍下相关的调度算法：O1和CFS。O1调度算法是在Linux 2.6开始引入的，到Linux 2.6.23之后内核将调度算法替换成了CFS。

### O1

#### 思路

**O1调度器仍然是根据经典的时间片分配的思路来进行整体设计的**。每个进程轮流占用cpu时间，而在这种情况下，如何支持优先级呢？实际上就是将时间片分配成大小不等的若干种，优先级高的进程使用大的时间片，优先级小的进程使用小的时间片。

O1算法有一个比较特殊的地方是，即使是相同的nice值的进程，也会再根据其CPU的占用情况将其分成两种类型：**CPU消耗型和IO消耗性**。随着其不断执行，内核会观察进程的CPU消耗状态，并**动态调整priority值**，可调整的范围是+-5。会适当调整IO消耗性的进程，使其优先级更高，提高交互响应速度。

#### 流程

1. 进程产生fork的时候会给一个进程分配一个时间片长度。这个新进程的时间片一般是父进程的一半，而父进程也会因此减少它的时间片长度为原来的一半。就是说，如果一个进程产生了子进程，那么它们将会平分当前时间片长度。

2. 新产生的进程都会先获得一个时间片，进入**可执行队列**等待调度到CPU执行。

3. 内核会在每个tick间隔期间（cpu时钟中断间隔，每秒钟会有1000个）对正在CPU上执行的进程进行检查，每个tick间隔周期主要检查两个内容：

   - 当前正在占用CPU的进程是不是时间片已经耗尽了？
   - 是不是有更高优先级的进程在活动队列中等待调度？（Linux支持抢占）

   如果任何一种情况成立，就把则当前进程的执行状态终止，放到**可执行队列**中，换当前在**可执行队列中优先级最高**的那个进程执行。

### CFS

O1已经是上一代调度器了，由于其对多核、多CPU系统的支持性能并不好，Linux在2.6.23之后开始启用CFS作为对一般优先级(SCHED_OTHER)进程调度方法。在这个重新设计的调度器中，时间片，动态、静态优先级以及IO消耗，CPU消耗的概念都不再重要。

#### 思路

CFS想要实现一个对**所有进程完全公平**的调度器。如果当前有n个进程需要调度执行，那么调度器应该在一个**比较小的时间范围**内，把这**n个进程全部**都调度执行一遍，并且它们**平分cpu时间**，这样就可以做到所有进程的公平调度。

这个较小的时间范围，就是调度周期（*sched_latency_ns*），任意一个就绪状态的进程，都一定会在这个调度周期内被调度，换一种说法就是每个进程等待CPU的时间最长不超过这个调度周期。

每个进程有一个vruntime，表示该进程历史已占用CPU的时间（并非实际值，只是一个权重）。vruntime越小，说明该进程越应该被调度，因此，每次都选择vruntime最小的进程进行执行。

具体实现时，CFS通过每个进程的虚拟运行时间（vruntime）来衡量哪个进程最值得被调度。CFS中的就绪队列是一棵以vruntime为键值的红黑树，虚拟时间越小的进程越靠近整个红黑树的最左端。因此，调度器每次选择位于红黑树最左端的那个进程，该进程的vruntime最小。

##### vruntime

vruntime（虚拟运行时间）是通过进程的**实际运行时间和进程的权重（weight）计算出来的**。在CFS调度器中，将进程优先级这个概念弱化，而是**强调进程的权重**。一个进程的权重越大，则说明这个进程更需要运行，因此它的虚拟运行时间就越小，这样被调度的机会就越大。

那么，CFS调度器中的权重怎么计算出来的呢？内核根据“每差一级nice值cpu占用时间差10%左右”的原则，计算了40个nice值分别对应的权重，以prio_to_weight数组存在：

```c
static const int prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};
```

进程的vruntime是自进程产生以来进行累加的，每个时钟周期内一个进程的vruntime是通过下面的方法计算的：

```
一次调度间隔的vruntime = 实际运行时间*（NICE_0_LOAD/权重）
                     = 实际运行时间 * 1024 / 进程权重

NICE_0_LOAD = 1024, 表示nice值为0的进程权重
```

可以看出，nice值为0的进程实际运行时间和虚拟运行时间相同。通过这个公式可以看到，权重越大的进程获得的虚拟运行时间越小，那么它将被调度器所调度的机会就越大。

> 调度器总是选择vruntime最小的进程进行调度。那么如果有两个进程的初始化vruntime时间一样时，一个进程被选择进行调度处理，那么只要一进行处理，它的vruntime时间就会大于另一个进程，CFS难道要马上换另一个进程处理么？出于减少频繁切换进程所带来的成本考虑，显然并不应该这样。
>
> CFS设计了一个sched_min_granularity_ns参数，用来设定进程被调度执行之后的最小CPU占用时间。一个进程被调度执行后至少要被执行这么长时间才会发生调度切换。

##### 新进程的VRUNTIME值

CFS是通过vruntime最小值来选择需要调度的进程的，在一个已经有多个进程执行了相对较长的系统中，这个队列中的vruntime时间纪录的数值都会比较长。

如果新产生的进程直接将自己的vruntime值设置为0的话，那么它将在执行开始的时间内抢占很多的CPU时间，直到自己的vruntime追赶上其他进程后才可能调度其他进程，这种情况显然是不公平的。

所以CFS对每个CPU的执行队列都维护一个**min_vruntime**值，这个值纪录了这个CPU执行队列中vruntime的最小值，当队列中出现一个新建的进程时，它的初始化vruntime将不会被设置为0，而是根据min_vruntime的值为基础来设置。这样就保证了新建进程的vruntime与老进程的差距在一定范围内，不会因为vruntime设置为0而在进程开始的时候占用过多的CPU。

##### IO消耗型进程的处理

如果CFS采用默认的策略处理**IO消耗型进程**的话，相比CPU消耗程序来说，这些应用由于绝大多数时间都处在sleep状态，它们的vruntime时间基本是不变的，一旦它们进入了调度队列，将会很快被选择调度执行。

被唤醒之后，这个进程的vruntime时间就可能会比别人小很多，从而导致不公平，所以对于这样的进程，CFS也会对其进行时间补偿。

补偿方式为，如果进程是从sleep状态被唤醒的，而且GENTLE_FAIR_SLEEPERS属性的值为true，则vruntime被设置为sched_latency_ns的一半和当前进程的vruntime值中比较大的那个。

##### 多CPU的负载均衡

CFS对每个CPU核心都维护一个调度队列，这样每个CPU都对自己的队列进程调度即可。这也是CFS比O1调度算法更高效的根本原因：每个CPU一个队列，就可以避免对全局队列使用大内核锁，从而提高了并行效率。

当然，这样最直接的影响就是CPU之间的负载可能不均，为了维持CPU之间的负载均衡，CFS要定期对所有CPU进行load balance操作，于是就有可能发生进程在不同CPU的调度队列上切换的行为。这种操作的过程也需要对相关的CPU队列进行锁操作，从而降低了多个运行队列带来的并行性。不过总的来说，CFS的并行队列方式还是要比O1的全局队列方式要高效。

在CFS对不同CPU的调度队列做均衡的时候，可能会将某个进程切换到另一个CPU上执行。此时，CFS会在将这个进程出队的时候将vruntime减去当前队列的min_vruntime，其差值作为结果会在入队另一个队列的时候再加上所入队列的min_vruntime，以此来保持队列切换后CPU队列的相对公平。

#### 执行过程

1. 找到下一个节点：红黑树上键值最小的那个节点：就是最左叶子节点。
2. 向队列中加入新进程：本质就是往红黑树加入新节点。这会发生在两种情况下：一是当进程由阻塞态被唤醒，二是fork产生新的进程时。
3. 从队列中移除进程：从队列中删除一个节点有两种可能：一是进程执行完毕退出，而是进程受到了阻塞。
4. 和O1一样，内核会在每个tick间隔期间对正在CPU上执行的进程进行检查，每个tick间隔周期检查当前进程的vruntime是否是比红黑树上最小的还小，如果不是，则选择红黑树上最小的来执行（抢占），就是最左叶子节点。

## 调度时机

1. 主动让出。当前进程（正在CPU上运行的进程）状态变为非可执行状态：sleep，exit，IO等
2. 抢占。进程运行时，非预期地被剥夺CPU的使用权。这又分两种情况：进程用完了时间片、或出现了优先级更高的进程。

## 上下文切换

上下文切换是指从一个可执行进程切换到另一个可执行进程，由`context_switch()`实现。

过程：

1. 把虚拟内存从上一个内存映射切换到新进程中，也就是task_struct->mm（用户态进程栈）
2. 切换处理器状态到新进程，这包括保存、恢复寄存器（PC、PSW等）和栈（内核态进程栈）的相关信息

带来的消耗：

直接的消耗包括CPU寄存器需要保存和加载，系统调度器的代码需要执行。

间接消耗在于多核cache之间的共享数据。 

## 抢占

### 用户抢占

内核即将返回用户空间的时候，如果`need_reshced`标志位被设置，会导致`schedule()`被调用，此时就发生了用户抢占。意思是说，既然要重新进行调度，那么可以继续执行进入内核态之前的那个进程，也完全可以重新选择另一个进程来运行，所以如果设置了`need_resched`，内核就会选择一个更合适的进程投入运行。

 简单来说有以下两种情况会发生用户抢占：

- 从系统调用返回用户空间
- 从中断处理程序返回用户空间

### 内核抢占

Linux和其他大部分的Unix变体操作系统不同的是，它支持完整的内核抢占。

不支持内核抢占的系统意味着：内核代码可以一直执行直到它完成为止，内核级的任务执行时无法重新调度，各个任务是以协作方式工作的，并不存在抢占的可能性。

下面罗列可能的内核抢占情况：

- 中断处理正在执行，且返回内核空间之前
- 内核代码再一次具有可抢占性时
- 内核中的任务显式地调用`schedule()`
- 内核中的任务被阻塞

## 参考

[深入 Linux 的进程优先级](https://linux.cn/article-7325-1.html)

[CFS中的虚拟运行时间](http://edsionte.com/techblog/archives/4331)